{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "-QXpz4OKlz3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08m3iJBTq5S0"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=256, num_layers=2,bidirectional=False,dropout_rate=0.5,fc_units = 512):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers,dropout = dropout_rate,batch_first=True,bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(hidden_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, fc_units//2)\n",
        "        self.fc3 = nn.Linear(fc_units//2, fc_units//4)\n",
        "        self.fc4 = nn.Linear(fc_units//4, 1)\n",
        "\n",
        "        if(self.bidirectional == True):\n",
        "            self.fc1 = nn.Linear(hidden_size*2, fc_units*2)\n",
        "            self.fc2 = nn.Linear(fc_units*2, fc_units)\n",
        "            self.fc3 = nn.Linear(fc_units, fc_units//2)\n",
        "            self.fc4 = nn.Linear(fc_units//2, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(self.bidirectional == True):\n",
        "            direction = 2\n",
        "        else:\n",
        "            direction = 1\n",
        "\n",
        "        h0 = torch.zeros(direction*self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(direction*self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _  = self.lstm(x, (h0, c0))\n",
        "        out_fc1 = self.fc1(out[:, -1, :])  # Use only the last output\n",
        "        out_fc1 = self.relu(out_fc1)\n",
        "        out_fc2 = self.fc2(self.dropout(out_fc1))\n",
        "        out_fc2 = self.relu(out_fc2)\n",
        "        out_fc3 = self.fc3(self.dropout(out_fc2))\n",
        "        out_fc3 = self.relu(out_fc3)\n",
        "        out_fc4 = self.fc4(self.dropout(out_fc3))\n",
        "        output = self.sigmoid(out_fc4)\n",
        "        output = output.view((len(x)))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOnuVnnhAkaR"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=256, num_layers=2,bidirectional=False,dropout_rate = 0.5,fc_units = 512):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size = input_size, hidden_size = hidden_size, num_layers=num_layers, dropout = dropout_rate,batch_first=True,bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(hidden_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, fc_units//2)\n",
        "        self.fc3 = nn.Linear(fc_units//2, fc_units//4)\n",
        "        self.fc4 = nn.Linear(fc_units//4, 1)\n",
        "\n",
        "        if(self.bidirectional == True):\n",
        "            self.fc1 = nn.Linear(hidden_size*2, fc_units*2)\n",
        "            self.fc2 = nn.Linear(fc_units*2, fc_units)\n",
        "            self.fc3 = nn.Linear(fc_units, fc_units//2)\n",
        "            self.fc4 = nn.Linear(fc_units//2, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(self.bidirectional == True):\n",
        "            direction = 2\n",
        "        else:\n",
        "            direction = 1\n",
        "\n",
        "        h0 = torch.zeros(direction*self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out_fc1 = self.fc1(out[:, -1, :])  # Use only the last output\n",
        "        out_fc1 = self.relu(out_fc1)\n",
        "        out_fc2 = self.fc2(self.dropout(out_fc1))\n",
        "        out_fc2 = self.relu(out_fc2)\n",
        "        out_fc3 = self.fc3(self.dropout(out_fc2))\n",
        "        out_fc3 = self.relu(out_fc3)\n",
        "        output = self.fc4(self.dropout(out_fc3))\n",
        "        output = self.sigmoid(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2TWExFMAoj9"
      },
      "outputs": [],
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_size=1,sequence_length=800,fc_units=512,dropout_rate= 0.4):\n",
        "        super(CNN1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=60, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=60, out_channels=40, kernel_size=3)\n",
        "        self.conv3 = nn.Conv1d(in_channels=40, out_channels=20, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(in_channels=20, out_channels=10, kernel_size=3)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Calculate input size for the fully connected layer\n",
        "        conv_output_size = self._get_conv_output_size(input_size, sequence_length)\n",
        "        self.fc1 = nn.Linear(conv_output_size, fc_units)\n",
        "\n",
        "        self.fc2 = nn.Linear(fc_units, fc_units//2)\n",
        "        self.fc3 = nn.Linear(fc_units//2, fc_units//4)\n",
        "        self.fc4 = nn.Linear(fc_units//4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def _get_conv_output_size(self, input_size, sequence_length):\n",
        "        # Calculate the output size after passing through the convolutional layers and pooling layer\n",
        "        x = torch.randn(256, input_size, sequence_length)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "\n",
        "        return x.size(1) * x.size(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Forward pass through fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(self.dropout(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(self.dropout(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(self.dropout(x))\n",
        "\n",
        "        # Apply sigmoid activation function\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk3LnzYGN40Z"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim=1, d_model=64, nhead=8, num_encoder_layers=6, dim_feedforward=256,fc_units=512, dropout_rate=0.3, max_len=800):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout_rate,batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
        "        self.d_model = d_model\n",
        "        self.fc1 = nn.Linear(d_model, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, fc_units//2)\n",
        "        self.fc3 = nn.Linear(fc_units//2, fc_units//4)\n",
        "        self.fc4 = nn.Linear(fc_units//4, 1)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = torch.mean(x,dim=1)  # Global average pooling\n",
        "        x = self.fc1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        output = self.sigmoid(x)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=800):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:,:x.size(1), :]"
      ]
    }
  ]
}